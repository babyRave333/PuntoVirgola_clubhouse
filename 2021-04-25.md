# A.G.I - "Stop Button" Problem

**A.G.I.**

    -> Artificial General Intelligence

    -> AGI che impara e si muove nel'ambiente come gli esseri umani

    -> al momento non ne esistono poichè non sappiamo come controllarle



Le AGI sono così intelligenti che non riusciamo a prevedere il comportamento

**Russel** -> ipotizza che se creiamo delle AGI per far provare più piacere possibile, potrebbe arrivare ad impiantare dei chip nel cervello per indurlo a provare emozioni

**Utility function** -> da punteggi positivi ogni volta che l'AGI fa qualcosa per raggiungere l'obiettivo e negativi ogni volta che fa qualcosa che lo allontana dall'obiettivo

**Corregibility** -> saper di non essere perfetti e di potersi migliorare

**Leggi della robotica di Asimov** -> ✨inutili✨

**Stop button** -> serve per spegnere l'AGI

L'AGI non permetterà di premere il bottone, poichè lo allontana dalla task

---

### Ipotetiche soluzioni

**Test 1**:
viene assegnato lo stesso punteggio sia allo stop button sia alla task -> minima spesa, massima resa -> cerca di premere il button

**Test 2**:
stop button che l'AGI non può premere -> passa tutti i **test, **ma non possiamo sapere cosa farà in una situazione reale

**Test 3**:
l'AGI non sa che esiste lo stop button -> inizialmente non sa che esiste, ma quando viene rilasciato nel mondo reale, ha l'intelligenza umana e di conseguenza in qualche modo lo scoprirà

**Test 4**:
lo programmiamo per non imparare niente riguardante il bottone -> teoricamente geniale, in pratica è molto difficile da implementare

---

### Reinforcemente Learning
Pratica molto utilizzata nei giochi (es. scacchi -> [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo))

**agent** -> AGI

**enviroment** -> campo da basket

**utility function** -> mettere la palla nel canestro

**epoche** -> periodi in cui le AGI imparano

continua a migliorare

## REVERSE COOPERATIVE LEARNING -> solution to the Stop Button problem
Cambia il modo in cui viene addestrata l'AI

**agent** -> AGI

**enviroment** -> casa con esseri umani

**utility function** -> imparare quali sono gli obiettivi degli esseri umani e come vengono portati a termine

**problema** -> si richiede all'AGI di imparare ad essere il più etica possibile, quando neanche noi umani siamo umani abbiamo un'etica perfetta

